<!--
  Slide 18: Key Takeaways - Summary
  Time: 63:00-66:00
-->
<div class="space-y-0.5">
  <h4 class="text-sm font-bold text-center">What We've Learned</h4>
  <div class="bg-gradient-to-r from-purple-100 to-pink-100 p-0.5 rounded-lg border-3 border-purple-500">
    <div class="grid grid-cols-2 gap-0.5">
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight">✓ <span class="font-bold">Gradient</span> = slope of tangent line; direction
          of steepest increase</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight">✓ <span class="font-bold">Partial derivatives</span> measure change in one
          variable (others fixed)</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight">✓ <span class="font-bold">Gradient vector</span> contains all partial
          derivatives</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight">✓ <span class="font-bold">Convex functions</span> have one global
          minimum—perfect for GD!</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight">✓ <span class="font-bold">Directional derivative</span> in opposite gradient =
          steepest descent</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight">✓ <span class="font-bold">Learning rate</span> controls step size &
          convergence</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight">✓ <span class="font-bold">Gradient descent</span> trades exact solution for
          efficiency</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight">✓ <span class="font-bold">Foundation</span> of modern machine learning!</p>
      </div>
    </div>
  </div>
</div>