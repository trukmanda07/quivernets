<!--
  Slide 16: Finale - Complete Picture
  Time: 55:00-60:00
-->
<div class="space-y-0.5">
  <h4 class="text-sm font-bold text-center">Putting It All Together</h4>
  <div class="bg-gradient-to-r from-blue-100 to-purple-100 p-0.5 rounded-lg border-3 border-blue-500">
    <h5 class="text-3xl font-bold mb-0 text-center">The Complete Gradient Descent Picture</h5>
    <div class="grid grid-cols-2 gap-0.5">
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight"><span class="font-bold">1️⃣ Problem:</span><br>Normal equation: $O(n^3)$
          complexity—too expensive!</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight"><span class="font-bold">2️⃣ Solution:</span><br>Gradient descent—iterative
          optimization</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight"><span class="font-bold">3️⃣ How:</span><br>Start random → Compute gradient →
          Go downhill → Repeat</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight"><span class="font-bold">4️⃣ Why:</span><br>Convex function → Gradient opposite
          direction → Guaranteed convergence!</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight"><span class="font-bold">5️⃣ Key Param:</span><br>Learning rate $\varepsilon$
          controls step size</p>
      </div>
      <div class="bg-white p-0.5 rounded-lg shadow">
        <p class="text-2xl leading-tight"><span class="font-bold">6️⃣ Modern:</span><br>Adaptive methods (Adam, RMSprop)
          auto-tune!</p>
      </div>
    </div>
  </div>
</div>