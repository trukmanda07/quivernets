<!--
  Slide 1: Opening Image
  Time: 0:00-3:00
-->
<div class="space-y-1.5">
  <div class="bg-gradient-to-r from-purple-500 to-indigo-600 p-3 rounded-lg text-white">
    <h4 class="text-sm font-bold mb-1">The Problem: Finding Optimal Weights</h4>
    <p class="text-2xl">We learned the normal equation gives us exact solutions for linear regression...</p>
  </div>
  <div class="bg-white p-2.5 rounded-lg shadow-lg border-4 border-purple-500">
    <p class="text-2xl mb-1 text-center font-bold">Normal Equation:</p>
    <p class="text-3xl font-mono text-center">$W = (X^TX)^{-1}X^Ty$</p>
    <p class="text-2xl text-center mt-1.5">‚úì Exact solution in one step</p>
  </div>
  <div class="bg-red-100 border-l-4 border-red-500 p-2.5">
    <p class="text-2xl font-bold mb-1">‚ö†Ô∏è But there's a catch:</p>
    <p class="text-2xl leading-tight">Matrix inversion has $O(n^3)$ complexity<br>10,000 features = 1 trillion
      operations!<br>This becomes impractical VERY quickly.</p>
  </div>
  <div class="bg-gradient-to-r from-green-500 to-emerald-500 p-2.5 rounded-lg text-white text-center">
    <p class="text-2xl font-bold">üí° The Alternative: Gradient Descent</p>
    <p class="text-2xl mt-0.5">Take many small, cheap steps instead of one expensive leap!</p>
  </div>
</div>