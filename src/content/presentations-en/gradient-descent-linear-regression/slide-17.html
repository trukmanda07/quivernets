<!--
  Slide 17: Comparison - Methods Side-by-Side
  Time: 60:00-63:00
-->
<div class="space-y-1">
  <h4 class="text-sm font-bold">Normal Equation vs. Gradient Descent</h4>
  <div class="grid grid-cols-2 gap-1.5">
    <div class="bg-blue-100 p-2 rounded-lg border-4 border-blue-500">
      <h5 class="text-3xl font-bold mb-0.5">Normal Equation</h5>
      <p class="text-2xl font-mono text-center mb-1">$W = (X^TX)^{-1}X^Ty$</p>
      <p class="text-2xl mb-0.5 font-bold text-green-700">✓ Advantages:</p>
      <p class="text-2xl leading-tight">Exact solution in one step<br>No parameters to tune<br>No iterations needed</p>
      <p class="text-2xl mt-1 mb-0.5 font-bold text-red-700">✗ Disadvantages:</p>
      <p class="text-2xl leading-tight">$O(n^3)$ complexity<br>Requires matrix inversion<br>Impractical for large $n$
      </p>
      <p class="text-2xl mt-1 font-semibold">Use: $n < 10,000$</p>
    </div>
    <div class="bg-green-100 p-2 rounded-lg border-4 border-green-500">
      <h5 class="text-3xl font-bold mb-0.5">Gradient Descent</h5>
      <p class="text-2xl font-mono text-center mb-1">$W_{new} = W_{old} - \varepsilon \nabla_W f(W)$</p>
      <p class="text-2xl mb-0.5 font-bold text-green-700">✓ Advantages:</p>
      <p class="text-2xl leading-tight">Scales to large datasets<br>$O(n)$ per iteration<br>Memory
        efficient<br>Foundation for deep learning</p>
      <p class="text-2xl mt-1 mb-0.5 font-bold text-red-700">✗ Disadvantages:</p>
      <p class="text-2xl leading-tight">Many iterations needed<br>Must tune learning rate</p>
      <p class="text-2xl mt-1 font-semibold">Use: $n > 10,000$</p>
    </div>
  </div>
</div>