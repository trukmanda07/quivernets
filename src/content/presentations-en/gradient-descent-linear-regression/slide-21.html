<!--
  Slide 21: What's Next
  Time: 70:00-72:00
-->
<div class="space-y-1.5">
  <h4 class="text-sm font-bold text-center">Continue Your Learning Journey</h4>
  <div class="bg-gradient-to-r from-blue-500 to-purple-600 p-3 rounded-lg text-white shadow-xl">
    <h5 class="text-3xl font-bold mb-1.5 text-center">Ready to Explore More?</h5>
    <div class="space-y-1">
      <div class="bg-white/20 p-2 rounded-lg backdrop-blur">
        <p class="text-2xl font-bold">ğŸ¯ Stochastic Gradient Descent (SGD)</p>
        <p class="text-2xl">Computing gradients on mini-batches instead of entire dataset</p>
      </div>
      <div class="bg-white/20 p-2 rounded-lg backdrop-blur">
        <p class="text-2xl font-bold">ğŸš€ Advanced Optimizers</p>
        <p class="text-2xl">Deep dive into Adam, RMSprop, and momentum methods</p>
      </div>
      <div class="bg-white/20 p-2 rounded-lg backdrop-blur">
        <p class="text-2xl font-bold">ğŸ›¡ï¸ Regularization</p>
        <p class="text-2xl">L1 and L2 penalties to prevent overfitting</p>
      </div>
      <div class="bg-white/20 p-2 rounded-lg backdrop-blur">
        <p class="text-2xl font-bold">ğŸ§  Non-linear Models</p>
        <p class="text-2xl">Extending these concepts to neural networks!</p>
      </div>
    </div>
  </div>
</div>