<!--
  Slide 11: Break into Two - The Algorithm
  Time: 36:00-40:00
-->
<div class="space-y-1">
  <h4 class="text-sm font-bold">The Gradient Descent Algorithm</h4>
  <div class="bg-gradient-to-r from-blue-500 to-indigo-600 p-2 rounded-lg text-white shadow-xl">
    <h5 class="text-3xl font-bold mb-1 text-center">üéØ Key Insight</h5>
    <p class="text-2xl text-center">Move in the OPPOSITE direction of the gradient!</p>
    <p class="text-2xl text-center mt-0.5 font-bold">This is why it's called gradient DESCENT! ‚¨áÔ∏è</p>
  </div>
  <div class="bg-white p-2 rounded-lg shadow-lg border-4 border-blue-500">
    <h5 class="text-3xl font-bold mb-1">Update Rule</h5>
    <div class="bg-gray-100 p-1.5 rounded-lg">
      <p class="text-2xl mb-0.5">General form:</p>
      <p class="text-2xl font-mono text-center">$\mathbf{x}_{new} = \mathbf{x}_{old} - \varepsilon
        \nabla_\mathbf{x}f(\mathbf{x}_{old})$</p>
      <p class="text-2xl mt-1 mb-0.5">For linear regression weights:</p>
      <p class="text-2xl font-mono text-center">$W_{new} = W_{old} - \varepsilon \nabla_W f(W_{old})$</p>
      <p class="text-2xl mt-1 text-center">where $\varepsilon$ is the <span class="font-bold text-blue-700">learning
          rate</span></p>
    </div>
  </div>
</div>