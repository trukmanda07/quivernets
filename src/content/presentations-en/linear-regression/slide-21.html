<!--
  Slide 21: Break into Three: The Solution
  Time: 61:00-64:00
-->
<div class="space-y-1.5">
  <h4 class="text-sm font-bold text-center text-green-700">ğŸ’¡ Iterative Optimization!</h4>
  <div class="bg-gradient-to-br from-green-50 to-emerald-50 p-3 rounded-lg border-4 border-green-500">
    <h5 class="text-3xl font-bold mb-1">The Alternative: Gradient Descent</h5>
    <p class="text-2xl mb-1.5">Instead of computing the inverse directly, take <strong>small steps</strong> toward the
      minimum!</p>
    <div class="bg-white p-2 rounded-lg shadow-lg">
      <h5 class="text-3xl font-bold mb-1">How It Works</h5>
      <div class="text-2xl leading-tight">1ï¸âƒ£ Start with random weights<br>2ï¸âƒ£ Calculate gradient (direction of steepest
        increase)<br>3ï¸âƒ£ Move in opposite direction (downhill)<br>4ï¸âƒ£ Repeat until we reach the minimum</div>
    </div>
  </div>
  <div class="bg-blue-100 border-l-4 border-blue-500 p-2">
    <p class="text-2xl font-semibold">ğŸ¯ Advantages:</p>
    <p class="text-2xl leading-tight">âœ“ No matrix inversion needed<br>âœ“ Scales to millions of data points<br>âœ“
      Foundation of modern ML<br>âœ“ Works for non-linear models too!</p>
  </div>
</div>